{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRjmnUyi0dAVVWPIjaMs88",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TenetiSrujana/Grocery_store/blob/main/CBIT_Winter_Upskilling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CBIT - Winter UpSkilling -> *Assignment*-1**"
      ],
      "metadata": {
        "id": "QGWzukVLdPHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.Simulate a dataset for Classification with 3 Labels, 6 attributes with interaction among them,10000 instances and use Uniform random function"
      ],
      "metadata": {
        "id": "LHpsNidpeYiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "samples = 10000\n",
        "features = 6\n",
        "classes = 3\n",
        "\n",
        "# Using uniform random function\n",
        "x = np.random.uniform(low=-1, high=1, size=(samples, features))\n",
        "\n",
        "interaction = (x[:, 0] * x[:, 1]) + (x[:, 2] * x[:, 3]) + (x[:, 4] + x[:, 5])\n",
        "\n",
        "y = np.random.randint(0, classes, size=samples)\n",
        "\n",
        "df = pd.DataFrame(x, columns=[\"Feature_1\", \"Feature_2\", \"Feature_3\", \"Feature_4\", \"Feature_5\", \"Feature_6\"])\n",
        "df[\"Target\"] = y\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, :-1], df[\"Target\"], test_size=0.3, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "lAqje6LvsGcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.Simulate a dataset for regression 6 attributes with interaction among them, 10000 instances and use normal random function"
      ],
      "metadata": {
        "id": "uuN6NSP_sHfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "samples = 10000\n",
        "features = 6\n",
        "\n",
        "# Using normal random function\n",
        "x = np.random.normal(loc=0, scale=1, size=(samples, features))\n",
        "\n",
        "interaction = (x[:, 0] * x[:, 1]) + (x[:, 2] * x[:, 3]) + (x[:, 4] + x[:, 5])\n",
        "\n",
        "y = interaction\n",
        "\n",
        "df = pd.DataFrame(x, columns=[\"Feature_1\", \"Feature_2\", \"Feature_3\", \"Feature_4\", \"Feature_5\", \"Feature_6\"])\n",
        "df[\"Target\"] = y\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, :-1], df[\"Target\"], test_size=0.3, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "XHRLeO58ej4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5.Simulate a dataset for Clustering 6 attributes with interaction among them, 10000 instances and use normal random function."
      ],
      "metadata": {
        "id": "uI4U57e3sM3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "samples = 10000\n",
        "features = 6\n",
        "\n",
        "# Using normal random function\n",
        "x = np.random.normal(loc=0, scale=1, size=(samples, features))\n",
        "\n",
        "interaction = (x[:, 0] * x[:, 1]) + (x[:, 2] * x[:, 3]) + (x[:, 4] + x[:, 5])\n",
        "\n",
        "df = pd.DataFrame(x, columns=[\"Feature_1\", \"Feature_2\", \"Feature_3\", \"Feature_4\", \"Feature_5\", \"Feature_6\"])\n",
        "df[\"Interaction\"] = interaction\n",
        "\n",
        "X_train, X_test = train_test_split(df.iloc[:, :-1], test_size=0.3, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "PBwo5_QYsaM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.Build a Non Linear Regression model and get 90% Accuracy for the above simulated data in question2. Split the data into 70% and 30% respectively for Training and Testing for model building"
      ],
      "metadata": {
        "id": "OdM455d_sall"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "samples = 10000\n",
        "features = 6\n",
        "\n",
        "# Using normal random function\n",
        "x = np.random.normal(loc=0, scale=1, size=(samples, features))\n",
        "\n",
        "interaction = (x[:, 0] * x[:, 1]) + (x[:, 2] * x[:, 3]) + (x[:, 4] + x[:, 5])\n",
        "\n",
        "y = interaction\n",
        "\n",
        "df = pd.DataFrame(x, columns=[\"Feature_1\", \"Feature_2\", \"Feature_3\", \"Feature_4\", \"Feature_5\", \"Feature_6\"])\n",
        "df[\"Target\"] = y\n",
        "\n",
        "# training and testing sets (70% training, 30% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.drop(\"Target\", axis=1), df[\"Target\"], test_size=0.3, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Non-Linear Regression model using Decision Tree Regressor\n",
        "model = DecisionTreeRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Prediction\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Prediction:\", y_pred)"
      ],
      "metadata": {
        "id": "CuYncvs9sh6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7.Build any five Classifiers for the above dataset in question1 and compare the metrics of these built models. Metrics need to build are Accuracy, Precision, Recall, F1Score. Split the data into 70% and 30% respectively for Training and Testing for model building"
      ],
      "metadata": {
        "id": "-1EkhQ3ZsjIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "samples = 10000\n",
        "features = 6\n",
        "classes = 3\n",
        "\n",
        "# Using normal random function\n",
        "x = np.random.normal(loc=0, scale=1, size=(samples, features))\n",
        "\n",
        "interaction = (x[:, 0] * x[:, 1]) + (x[:, 2] * x[:, 3]) + (x[:, 4] + x[:, 5])\n",
        "\n",
        "y = np.digitize(interaction, bins=np.percentile(interaction, [33, 66])) % classes\n",
        "\n",
        "df = pd.DataFrame(x, columns=[\"Feature_1\", \"Feature_2\", \"Feature_3\", \"Feature_4\", \"Feature_5\", \"Feature_6\"])\n",
        "df[\"Target\"] = y\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.drop(\"Target\", axis=1), df[\"Target\"], test_size=0.3, random_state=42)\n",
        "\n",
        "#  5-classifiers\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=10000),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"SVM\": SVC(),\n",
        "    \"KNN\": KNeighborsClassifier()\n",
        "}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(f\"\\n{model_name}:\")\n",
        "    print(f\"  Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
        "    print(f\"  Precision: {precision_score(y_test, y_pred, average='weighted')}\")\n",
        "    print(f\"  Recall: {recall_score(y_test, y_pred, average='weighted')}\")\n",
        "    print(f\"  F1-Score: {f1_score(y_test, y_pred, average='weighted')}\")"
      ],
      "metadata": {
        "id": "s5Kkwl6nsuU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8.Load CSV, EXCEL, EXCELS, JSON format datasets from local host and Websites into Colab interface and split them into Train and test data"
      ],
      "metadata": {
        "id": "bJ3eAZ7Ys37q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load CSV file\n",
        "df_csv = pd.read_csv('/content/Social_Network_Ads.csv')\n",
        "\n",
        "# Load Excel file\n",
        "df_excel = pd.read_excel('/content/SaleData.xlsx')\n",
        "\n",
        "X_train_csv, X_test_csv, y_train_csv, y_test_csv = train_test_split(df_csv.iloc[:, :-1], df_csv.iloc[:, -1], test_size=0.3, random_state=0)\n",
        "\n",
        "# Load multiple Excel sheets\n",
        "df_excels = pd.read_excel('/content/SaleData.xlsx', sheet_name=None)\n",
        "\n",
        "for sheet_name, sheet_df in df_excels.items():\n",
        "    X_train_excel, X_test_excel, y_train_excel, y_test_excel = train_test_split(sheet_df.iloc[:, :-1], sheet_df.iloc[:, -1], test_size=0.3, random_state=0)\n",
        "\n",
        "# Load JSON file\n",
        "df_json = pd.read_json('/content/Social_Network_Ads_yourgpt.json')\n",
        "X_train_json, X_test_json, y_train_json, y_test_json = train_test_split(df_json.iloc[:, :-1], df_json.iloc[:, -1], test_size=0.3, random_state=0)\n",
        "\n",
        "# Load CSV file\n",
        "df_csv_url = pd.read_csv('https://people.sc.fsu.edu/~jburkardt/data/csv/addresses.csv')\n",
        "X_train_csv_url, X_test_csv_url, y_train_csv_url, y_test_csv_url = train_test_split(df_csv_url.iloc[:, :-1], df_csv_url.iloc[:, -1], test_size=0.3, random_state=0)\n",
        "\n",
        "# Load Excel file\n",
        "df_excel_url = pd.read_excel('/content/SaleData.xlsx')\n",
        "X_train_excel_url, X_test_excel_url, y_train_excel_url, y_test_excel_url = train_test_split(df_excel_url.iloc[:, :-1], df_excel_url.iloc[:, -1], test_size=0.3, random_state=0)\n",
        "\n",
        "#  JSON file from a URL\n",
        "df_json_url = pd.read_json('/content/Social_Network_Ads_yourgpt.json')  # Replace with actual URL for JSON\n",
        "X_train_json_url, X_test_json_url, y_train_json_url, y_test_json_url = train_test_split(df_json_url.iloc[:, :-1], df_json_url.iloc[:, -1], test_size=0.3, random_state=0)\n",
        "\n",
        "# You can print or analyze the shapes of your splits\n",
        "print(f\"CSV Data Train-Test split: {X_train_csv.shape}, {X_test_csv.shape}\")\n",
        "print(f\"Excel Data Train-Test split: {X_train_excel.shape}, {X_test_excel.shape}\")\n",
        "print(f\"JSON Data Train-Test split: {X_train_json.shape}, {X_test_json.shape}\")"
      ],
      "metadata": {
        "id": "ztkBiXqCtVTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9.Load different datasets from SkLearn library into Colab interface and split them into train and test data. Apply Normalization techniques to convert input data into standardize\n"
      ],
      "metadata": {
        "id": "B4uLJ_gwtANY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris, load_digits, load_wine\n",
        "\n",
        "# Iris dataset\n",
        "iris = load_iris()\n",
        "X_iris = iris.data\n",
        "y_iris = iris.target\n",
        "\n",
        "# Digits dataset\n",
        "digits = load_digits()\n",
        "X_digits = digits.data\n",
        "y_digits = digits.target\n",
        "\n",
        "# Wine dataset\n",
        "wine = load_wine()\n",
        "X_wine = wine.data\n",
        "y_wine = wine.target\n",
        "\n",
        "# Split the Iris dataset\n",
        "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(X_iris, y_iris, test_size=0.3, random_state=0)\n",
        "\n",
        "# Split the Digits dataset\n",
        "X_train_digits, X_test_digits, y_train_digits, y_test_digits = train_test_split(X_digits, y_digits, test_size=0.3, random_state=0)\n",
        "\n",
        "# Split the Wine dataset\n",
        "X_train_wine, X_test_wine, y_train_wine, y_test_wine = train_test_split(X_wine, y_wine, test_size=0.3, random_state=0)\n",
        "\n",
        "# Normalize the Iris dataset\n",
        "scaler = StandardScaler()\n",
        "X_train_iris_scaled = scaler.fit_transform(X_train_iris)\n",
        "X_test_iris_scaled = scaler.transform(X_test_iris)\n",
        "\n",
        "# Normalize the Digits dataset\n",
        "X_train_digits_scaled = scaler.fit_transform(X_train_digits)\n",
        "X_test_digits_scaled = scaler.transform(X_test_digits)\n",
        "\n",
        "# Normalize the Wine dataset\n",
        "X_train_wine_scaled = scaler.fit_transform(X_train_wine)\n",
        "X_test_wine_scaled = scaler.transform(X_test_wine)\n",
        "\n",
        "print(f\"Iris Train data shape: {X_train_iris.shape}, Test data shape: {X_test_iris.shape}\")\n",
        "print(f\"Digits Train data shape: {X_train_digits.shape}, Test data shape: {X_test_digits.shape}\")\n",
        "print(f\"Wine Train data shape: {X_train_wine.shape}, Test data shape: {X_test_wine.shape}\")\n",
        "\n",
        "print(f\"Normalized Iris Train data shape: {X_train_iris_scaled.shape}, Normalized Test data shape: {X_test_iris_scaled.shape}\")\n",
        "print(f\"Normalized Digits Train data shape: {X_train_digits_scaled.shape}, Normalized Test data shape: {X_test_digits_scaled.shape}\")\n",
        "print(f\"Normalized Wine Train data shape: {X_train_wine_scaled.shape}, Normalized Test data shape: {X_test_wine_scaled.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4QmX03ltH2c",
        "outputId": "5cc9a115-7eb6-4d15-9803-7584bf0215e7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iris Train data shape: (105, 4), Test data shape: (45, 4)\n",
            "Digits Train data shape: (1257, 64), Test data shape: (540, 64)\n",
            "Wine Train data shape: (124, 13), Test data shape: (54, 13)\n",
            "Normalized Iris Train data shape: (105, 4), Normalized Test data shape: (45, 4)\n",
            "Normalized Digits Train data shape: (1257, 64), Normalized Test data shape: (540, 64)\n",
            "Normalized Wine Train data shape: (124, 13), Normalized Test data shape: (54, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#10.Load any data from website/Local host and apply the below data wrangling techniques a. Split b) merge c) select rows by conditions d) select columns by conditions e) Rename columns f) Min, max, avg, varience g) describe h) size I) shape j) find unique values k)find missing values and fill them with average value/ mode value l) replace attribute value name with other name m)delete columns n) delete some rows o) join datasets(left,right) p) groups values by rows q) loop over columns r) group rows by time s)concatenating datasets t) append rows u)append columns w) reorder columns using pandas lib"
      ],
      "metadata": {
        "id": "vxqaGgUBtIap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv('/content/Social_Network_Ads.csv')\n",
        "\n",
        "# a) Split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, :-1], df.iloc[:, -1], test_size=0.3, random_state=0)\n",
        "\n",
        "# b) Merge\n",
        "df1 = df[['Age', 'EstimatedSalary']]\n",
        "df2 = df[['Gender', 'Purchased']]\n",
        "merged_df = pd.merge(df1, df2, left_index=True, right_index=True)\n",
        "\n",
        "# c) Select rows by conditions\n",
        "filtered_df = df[df['Age'] > 30]\n",
        "\n",
        "# d) Select columns by conditions\n",
        "selected_columns = df.loc[:, df.columns.str.startswith('E')]\n",
        "\n",
        "# e) Rename columns\n",
        "df.rename(columns={'Age': 'age'}, inplace=True)\n",
        "\n",
        "# f) Min, max, avg, variance\n",
        "min_value = df['age'].min()\n",
        "max_value = df['age'].max()\n",
        "avg_value = df['age'].mean()\n",
        "variance_value = df['age'].var()\n",
        "\n",
        "# g) Describe\n",
        "summary_stats = df.describe()\n",
        "\n",
        "# h) Size\n",
        "size = df.size\n",
        "\n",
        "# i) Shape\n",
        "shape = df.shape\n",
        "\n",
        "# j) unique value\n",
        "unique_values = df['Gender'].unique()\n",
        "\n",
        "# k) Find missing values and fill them with the mean\n",
        "missing_values = df.isnull().sum()\n",
        "numeric_df = df.select_dtypes(include=np.number)\n",
        "df[numeric_df.columns] = numeric_df.fillna(numeric_df.mean())\n",
        "\n",
        "# l) Replace attribute values\n",
        "df['Gender'].replace({'Male': 'M', 'Female': 'F'}, inplace=True)\n",
        "\n",
        "# m) Delete columns\n",
        "df.drop(columns=['EstimatedSalary'], inplace=True)\n",
        "\n",
        "# n) Delete rows\n",
        "df.drop(df[df['age'] < 25].index, inplace=True)\n",
        "\n",
        "# o) Join DataFrames\n",
        "df_left = pd.DataFrame({'Age': [22, 35, 55], 'Name': ['A', 'B', 'C']})\n",
        "df_right = pd.DataFrame({'Age': [22, 35, 45], 'Salary': [2000, 4000, 6000]})\n",
        "joined_df = pd.merge(df_left, df_right, on='Age', how='left')\n",
        "\n",
        "# p) Group values by rows (\n",
        "grouped_df = df.groupby('Gender')['age'].mean()\n",
        "\n",
        "# q) Loop over columns\n",
        "for col in df.columns:\n",
        "    print(f\"Column: {col}, Type: {df[col].dtype}\")\n",
        "\n",
        "'''r) Group rows by time\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "grouped_by_time = df.groupby(df['Date'].dt.year).mean() '''\n",
        "\n",
        "# s) Concatenate datasets\n",
        "df2 = pd.DataFrame({'Age': [28, 30], 'Gender': ['F', 'M'], 'Purchased': [1, 0]})\n",
        "concatenated_df = pd.concat([df, df2], ignore_index=True)\n",
        "\n",
        "# t) Append rows\n",
        "new_row = pd.DataFrame({'age': [21], 'Gender': ['F'], 'Purchased': [0]})\n",
        "df = pd.concat([df, new_row], ignore_index=True)\n",
        "\n",
        "# u) Append columns\n",
        "df['NewColumn'] = np.nan\n",
        "\n",
        "# w) Reorder columns\n",
        "df = df[['age', 'Purchased', 'Gender']]\n",
        "\n",
        "print(\"Summary Stats:\\n\", summary_stats)\n",
        "print(\"Size of DataFrame:\", size)\n",
        "print(\"Shape of DataFrame:\", shape)\n",
        "print(\"Unique values in 'Gender' column:\", unique_values)\n",
        "print(\"First few rows after renaming columns and manipulation:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "pEQ94TuutX4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11.Load the Images and audio files. Covert audio file into spectrogram. Apply it for a length of one minute telugu song. Convert a colour image into gray scale image."
      ],
      "metadata": {
        "id": "xJBtJFxltYmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import librosa\n",
        "import librosa.display\n",
        "\n",
        "image_path = '/content/colour_image.jpg'\n",
        "image = Image.open(image_path)\n",
        "\n",
        "# Load the audio file (1-minute Telugu song)\n",
        "audio_path = '/content/ScreenRecording_01-07-2025 21-37-10_1.wav'\n",
        "audio, sr = librosa.load(audio_path, sr=None)\n",
        "\n",
        "# Color Image\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.title('Color Image')\n",
        "plt.show()\n",
        "\n",
        "#Convert Audio File into Spectrogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "D = librosa.amplitude_to_db(np.abs(librosa.stft(audio)), ref=np.max)\n",
        "librosa.display.specshow(D, x_axis='time', y_axis='log', sr=sr)\n",
        "plt.colorbar(format='%+2.0f dB')\n",
        "plt.title('Spectrogram of Telugu Song')\n",
        "plt.show()\n",
        "\n",
        "# Convert the Color Image into Grayscale Image\n",
        "gray_image = image.convert('L')\n",
        "plt.imshow(gray_image, cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.title('Grayscale Image')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TjSgU6twtgw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#12.Load any numeric data and apply Ridge and Lasso regressions to get 90% accuracy"
      ],
      "metadata": {
        "id": "u42rOJI0thjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# numeric dataset (using diabetes dataset)\n",
        "data = load_diabetes()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "#  Normalize\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "#  Ridge Regression\n",
        "ridge_reg = Ridge(alpha=1.0)\n",
        "ridge_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "#  Lasso Regression\n",
        "lasso_reg = Lasso(alpha=0.1)\n",
        "lasso_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"Ridge Regression Coefficients:\")\n",
        "print(ridge_reg.coef_)\n",
        "\n",
        "print(\"\\nLasso Regression Coefficients:\")\n",
        "print(lasso_reg.coef_)"
      ],
      "metadata": {
        "id": "FgCOxxLVtuQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#13.Load Digits dataset and apply clustering models to get its metrics"
      ],
      "metadata": {
        "id": "pS6zE_NWtvDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# KMeans clustering\n",
        "kmeans = KMeans(n_clusters=10, random_state=42)\n",
        "kmeans_labels = kmeans.fit_predict(X_train)\n",
        "\n",
        "# Metrics for KMeans\n",
        "silhouette_kmeans = silhouette_score(X_train, kmeans_labels)\n",
        "ari_kmeans = adjusted_rand_score(y_train, kmeans_labels)\n",
        "\n",
        "print(f\"KMeans Silhouette Score: {silhouette_kmeans}\")\n",
        "print(f\"KMeans Adjusted Rand Index: {ari_kmeans}\")\n",
        "\n",
        "# Agglomerative Clustering\n",
        "agg_clustering = AgglomerativeClustering(n_clusters=10)\n",
        "agg_labels = agg_clustering.fit_predict(X_train)\n",
        "\n",
        "# Metrics for Agglomerative Clustering\n",
        "silhouette_agg = silhouette_score(X_train, agg_labels)\n",
        "ari_agg = adjusted_rand_score(y_train, agg_labels)\n",
        "\n",
        "print(f\"Agglomerative Clustering Silhouette Score: {silhouette_agg}\")\n",
        "print(f\"Agglomerative Clustering Adjusted Rand Index: {ari_agg}\")"
      ],
      "metadata": {
        "id": "qx_QKC2otz2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#14.Implement the techniques to handle imbalanced dataset to balanced dataset with binary class data"
      ],
      "metadata": {
        "id": "8O3dLAmwt0e3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target, name='Target')\n",
        "\n",
        "X_imbalanced = pd.concat([X[y == 0], X[y == 1].sample(frac=0.2, random_state=42)])\n",
        "y_imbalanced = pd.concat([y[y == 0], y[y == 1].sample(frac=0.2, random_state=42)])\n",
        "\n",
        "def plot_distribution(y_data, title):\n",
        "    plt.bar(y_data.value_counts().index, y_data.value_counts(), color=['blue', 'orange'])\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Class')\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks(ticks=[0, 1], labels=['Class 0', 'Class 1'])\n",
        "    plt.show()\n",
        "\n",
        "plot_distribution(y_imbalanced, \"Imbalanced Class Distribution\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_imbalanced, y_imbalanced, test_size=0.3, random_state=42)\n",
        "\n",
        "X_oversample = pd.concat([X_train, X_train[y_train == 1]])\n",
        "y_oversample = pd.concat([y_train, y_train[y_train == 1]])\n",
        "\n",
        "X_undersample = X_train[y_train == 0].sample(len(y_train[y_train == 1]), random_state=42)\n",
        "y_undersample = y_train[y_train == 0].sample(len(y_train[y_train == 1]), random_state=42)\n",
        "\n",
        "X_undersampled = pd.concat([X_undersample, X_train[y_train == 1]])\n",
        "y_undersampled = pd.concat([y_undersample, y_train[y_train == 1]])\n",
        "\n",
        "plot_distribution(y_oversample, \"Oversampled Class Distribution\")\n",
        "plot_distribution(y_undersampled, \"Undersampled Class Distribution\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "zSPXrQ9zt5uX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#15.Apply KNN classifier on appropriate data.Find best neighbourhood size, Indices of neighbourhood,distances of neighbourhood by taking a random new tuple."
      ],
      "metadata": {
        "id": "koWTIH5It6jA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "iris=datasets.load_iris()\n",
        "features=iris.data\n",
        "target=iris.target\n",
        "\n",
        "# standardizer\n",
        "standardizer=StandardScaler()\n",
        "features_standardized=standardizer.fit_transform(features)\n",
        "print(features_standardized[0])\n",
        "\n",
        "# random tupple is selected\n",
        "random_index = np.random.randint(0, len(features))  # Random index to select a sample\n",
        "random_tuple = features[random_index]\n",
        "\n",
        "# NearestNeighbors\n",
        "nearest_neighbors = NearestNeighbors(n_neighbors=5)\n",
        "nearest_neighbors.fit(features_standardized)\n",
        "\n",
        "distances, indices = nearest_neighbors.kneighbors([features_standardized[random_index]])\n",
        "\n",
        "print(f\"Random Tuple (Index {random_index}): {random_tuple}\")\n",
        "print(\"The distances are:\", distances)\n",
        "print(\"The indices of the nearest neighbors are:\", indices)\n",
        "\n",
        "# Use KNN Classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(features_standardized, target)\n",
        "\n",
        "prediction = knn.predict([features_standardized[random_index]])\n",
        "print(f\"Predicted class for the random tuple: {prediction[0]}\")\n",
        "print(f\"Actual class of the random tuple: {target[random_index]}\")"
      ],
      "metadata": {
        "id": "cKiJKBocuBbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#16.Find support vectors and their probabilities by applying SVM on any binary classification dataset.Also find the best model to generate support vectors. Get the metrics for this trained model.Represent its graph also."
      ],
      "metadata": {
        "id": "2qWk1y43uCkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "data = datasets.load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# SVM classifier\n",
        "svm_model = SVC(kernel='linear', probability=True)\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "support_vectors = svm_model.support_vectors_\n",
        "\n",
        "y_prob = svm_model.predict_proba(X_test)\n",
        "\n",
        "y_pred = svm_model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# decision boundary (2D)\n",
        "X_train_2D = X_train[:, :2]\n",
        "svm_model_2d = SVC(kernel='linear', probability=True)\n",
        "svm_model_2d.fit(X_train_2D, y_train)\n",
        "\n",
        "h = 0.02\n",
        "x_min, x_max = X_train_2D[:, 0].min() - 1, X_train_2D[:, 0].max() + 1\n",
        "y_min, y_max = X_train_2D[:, 1].min() - 1, X_train_2D[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "\n",
        "Z = svm_model_2d.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "plt.contourf(xx, yy, Z, alpha=0.8)\n",
        "plt.scatter(X_train_2D[:, 0], X_train_2D[:, 1], c=y_train, edgecolors='k', marker='o', s=50)\n",
        "plt.scatter(support_vectors[:, 0], support_vectors[:, 1], facecolors='none', edgecolors='r', s=100)\n",
        "plt.title('SVM Decision Boundary with Support Vectors')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g6iVU5xRuTrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#17.Load a mixed type features data and apply appropriate model.Predict class for any new tuple and its probabilities."
      ],
      "metadata": {
        "id": "J1g4FeHruP0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Titanic dataset\n",
        "data = pd.read_csv('/content/Titanic.csv')\n",
        "\n",
        "data['Age'] = data['Age'].fillna(data['Age'].mean())\n",
        "data['Embarked'] = data['Embarked'].fillna(data['Embarked'].mode()[0])\n",
        "\n",
        "# Label Encoding\n",
        "for column in data.select_dtypes(include=['object']).columns:\n",
        "    le = LabelEncoder()\n",
        "    data[column] = le.fit_transform(data[column])\n",
        "\n",
        "X = data.drop(columns=['Survived'])\n",
        "y = data['Survived']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#Random Forest Classifier\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_prob = model.predict_proba(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy}\")\n",
        "print(f\"Predicted Class: {y_pred[0]}\")\n",
        "print(f\"Predicted Probabilities: {y_pred_prob[0]}\")"
      ],
      "metadata": {
        "id": "xrQMUaD4ua_g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}